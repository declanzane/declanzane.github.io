@inproceedings{ye_mitigating_2023,
    title = {Mitigating {Transformer} {Overconfidence} via {Lipschitz} {Regularization}},
    booktitle = {The 39th {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
    author = {Ye, Wenqian and Ma, Yunsheng and Cao, Xu and Tang, Kun},
    month = may,
    year = {2023},
    selected = {true},
    abbr = {UAI},
    teaser = {cover_mitigating.png},
}

@article{ma_cemformer_2023,
    title = {CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers},
    author = {Yunsheng Ma and Wenqian Ye and Xu Cao and Amr Abdelraouf and Kyungtae Han and Rohit Gupta and Ziran Wang},
    journal = {arXiv:2305.07840},
    arxiv = {2305.07840},
    year = {2023},
    selected = {true},
    abbr = {Preprint},
    teaser = {cover_cemformer.png},
}

@inproceedings{ma_vit-dd_2023,
    title = {{ViT}-{DD}: {Multi}-{Task} {Vision} {Transformer} for {Semi}-{Supervised} {Driver} {Distraction} {Detection}},
    booktitle = {2023 {IEEE} {Intelligent} {Vehicles} {Symposium}},
    author = {Ma, Yunsheng and Wang, Ziran},
    year = {2023},
    selected = {true},
    abbr = {IEEE-IV},
    teaser = {cover_vit-dd.png},
    code = {https://github.com/PurdueDigitalTwin/ViT-DD},
    arxiv = {2209.09178},
}


@inproceedings{ma_m2dar_2023,
    title = {{M2DAR}: {Multi}-{View} {Multi}-{Scale} {Driver} {Action} {Recognition} with {Vision} {Transformer}},
    booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
    author = {Ma, Yunsheng and Yuan, Liangqi and Abdelraouf, Amr and Han, Kyungtae and Gupta, Rohit and Li, Zihao and Wang, Ziran},
    year = {2023},
    selected = {true},
    abbr = {CVPRW},
    teaser = {cover_m2dar.png},
    arxiv = {2305.08877},
    award = {Outstanding Student Presentation Award at NGTS-3},
    code = {https://github.com/PurdueDigitalTwin/M2DAR},
}

@inproceedings{yuan_peer--peer_2023,
    title = {Peer-to-{Peer} {Federated} {Continual} {Learning} for {Naturalistic} {Driving} {Action} {Recognition}},
    booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
    author = {Yuan, Liangqi and Ma, Yunsheng and Su, Lu and Wang, Ziran},
    year = {2023},
    selected = {false},
    abbr = {CVPRW},
    teaser = {cover_p2p.png},
    arxiv = {2304.07421},
}

@inproceedings{zhao_vaanet_2020,
    title = {An {End}-to-{End} {Visual}-{Audio} {Attention} {Network} for {Emotion} {Recognition} in {User}-{Generated} {Videos}},
    booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
    author = {Zhao*, Sicheng and Ma*, Yunsheng and Gu, Yang and Yang, Jufeng and Xing, Tengfei and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
    year = {2020},
    selected = {true},
    abbr = {AAAI Oral},
    pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/5364},
    code = {https://github.com/maysonma/VAANet},
    teaser = {cover_vaanet.png},
}
