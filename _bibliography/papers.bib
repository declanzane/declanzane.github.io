@inproceedings{ma_macp_2024,
    author = {Yunsheng Ma* and Juanwu Lu* and Can Cui and Sicheng Zhao and Xu Cao and Wenqian Ye and Ziran Wang},
    title = {MACP: Efficient Model Adaptation for Cooperative Perception},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
    year = {2024},
    abbr = {WACV},
    selected = {true},
    teaser = {cover_macp.png},
    abstract = {Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to ``see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs.},
    arxiv = {2310.16870},
    code = {https://github.com/PurdueDigitalTwin/MACP},
    website = {https://purduedigitaltwin.github.io/MACP/},
}

@inproceedings{ye_mitigating_2023,
    title = {Mitigating {Transformer} {Overconfidence} via {Lipschitz} {Regularization}},
    booktitle = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
    author = {Ye, Wenqian and Ma, Yunsheng and Cao, Xu and Tang, Kun},
    month = may,
    year = {2023},
    selected = {true},
    abbr = {UAI},
    teaser = {cover_mitigating.png},
    abstract = {Though Transformers have achieved promising results in many computer vision tasks, they tend to be over-confident in predictions, as the standard Dot Product Self-Attention (DPSA) can barely preserve distance for the unbounded input domain. In this work, we fill this gap by proposing a novel Lipschitz Regularized Transformer (LRFormer). Specifically, we present a new similarity function with the distance within Banach Space to ensure the Lipschitzness and also regularize the term by a contractive Lipschitz Bound. The proposed method is analyzed with a theoretical guarantee, providing a rigorous basis for its effectiveness and reliability. Extensive experiments conducted on standard vision benchmarks demonstrate that our method outperforms state-of-the-art single forward pass approaches in prediction, calibration, and uncertainty estimation.},
    arxiv = {2306.06849},
}

@inproceedings{ma_cemformer_2023,
    title = {CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers},
    author = {Yunsheng Ma and Wenqian Ye and Xu Cao and Amr Abdelraouf and Kyungtae Han and Rohit Gupta and Ziran Wang},
    booktitle = {IEEE International Conference on Intelligent Transportation Systems},
    arxiv = {2305.07840},
    year = {2023},
    selected = {true},
    abbr = {ITSC},
    teaser = {cover_cemformer.png},
}

@inproceedings{cui_radar_2023,
    title = {Radar {Enlighten} the {Dark}: {Enhancing} {Low}-{Visibility} {Perception} for {Automated} {Vehicles} with {Camera}-{Radar} {Fusion}},
    author = {Cui, Can and Ma, Yunsheng and Lu, Juanwu and Wang, Ziran},
    booktitle = {IEEE International Conference on Intelligent Transportation Systems},
    arxiv = {2305.17318},
    year = {2023},
    selected = {true},
    abbr = {ITSC},
    teaser = {cover_redformer.png},
}

@inproceedings{ma_m2dar_2023,
    title = {{M2DAR}: {Multi}-{View} {Multi}-{Scale} {Driver} {Action} {Recognition} with {Vision} {Transformer}},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
    author = {Ma, Yunsheng and Yuan, Liangqi and Abdelraouf, Amr and Han, Kyungtae and Gupta, Rohit and Li, Zihao and Wang, Ziran},
    year = {2023},
    selected = {false},
    abbr = {CVPRW},
    teaser = {cover_m2dar.png},
    arxiv = {2305.08877},
    award = {Outstanding Speaker Award at NGTS <a href="/news/2023-05-18-ngts3/">[News]</a>},
    code = {https://github.com/PurdueDigitalTwin/M2DAR},
}

@inproceedings{yuan_peer--peer_2023,
    title = {Peer-to-{Peer} {Federated} {Continual} {Learning} for {Naturalistic} {Driving} {Action} {Recognition}},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
    author = {Yuan, Liangqi and Ma, Yunsheng and Su, Lu and Wang, Ziran},
    year = {2023},
    selected = {false},
    abbr = {CVPRW},
    teaser = {cover_p2p.png},
    arxiv = {2304.07421},
}

@inproceedings{zhao_vaanet_2020,
    title = {An {End}-to-{End} {Visual}-{Audio} {Attention} {Network} for {Emotion} {Recognition} in {User}-{Generated} {Videos}},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    author = {Zhao*, Sicheng and Ma*, Yunsheng and Gu, Yang and Yang, Jufeng and Xing, Tengfei and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
    year = {2020},
    selected = {true},
    abbr = {AAAI Oral},
    pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/5364},
    code = {https://github.com/maysonma/VAANet},
    teaser = {cover_vaanet.png},
    arxiv = {2003.00832},
}
